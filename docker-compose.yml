version: '3'
services:
  zookeeper:
    image: wurstmeister/zookeeper
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: ["CMD", "bash", "-c", "echo 'ruok' | nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - flight_pipeline

  broker:
    image: wurstmeister/kafka
    hostname: broker
    container_name: broker
    env_file:  
      - .env
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_HOURS: 12  # Retain messages for 12 hrs
      KAFKA_JMX_PORT: 9094
      KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.rmi.port=9999 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false"
    healthcheck:
      test: ["CMD", "bash", "-c", "nc -z localhost 9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - flight_pipeline

  producer:
    build:
      context: ./kafka_producer             # Directory for Dockerfile
    volumes:
      - ./kafka_producer:/app                # Mounts the kafka_producer folder to /app
      - ./Airline_Delay_Cause.csv:/app/Airline_Delay_Cause.csv
    environment:
      KAFKA_BOOTSTRAP_SERVERS: broker:29092
    env_file:  
      - .env
    depends_on:
      broker:
        condition: service_healthy
    networks:
      - flight_pipeline

  spark:
    image: bitnami/spark:3.5.0
    container_name: spark
    build:
      context: ./spark_processing
    volumes:
      - ./spark_processing:/app 
    env_file:  
      - .env
    ports:
      - "4040:4040"
    environment:
      - SPARK_OPTS=--conf spark.jars.ivy=/tmp/.ivy2
      # AWS credentials can be added here when S3 integration is ready
      # - AWS_ACCESS_KEY_ID=your_access_key
      # - AWS_SECRET_ACCESS_KEY=your_secret_key
    command: >
      /opt/bitnami/spark/bin/spark-submit
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.kafka:kafka-clients:2.8.1
      --conf spark.jars.ivy=/tmp/.ivy2
      /app/spark_streaming.py

    depends_on:
      broker:
        condition: service_healthy
    networks:
      - flight_pipeline

networks:
  flight_pipeline:
    external: true
